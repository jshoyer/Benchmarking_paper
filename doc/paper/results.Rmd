---
title: "Results"
author: "JT McCrone"
date: "Febuary 15, 2016"
output: word_document
---
```{r.message=F,echo=F}
xamp_roc<-"figure 1"
miseq<-"figure 2"
Miseq<-"Figure 2"
hiseq<-"figure 3"
Hiseq<-"Figure 3"
qual<-"figure 4"
Qual<-"Figure 4"
freq<-"figure 5"
Freq<-"Figure 5"
lofreq<-"Figure 6"
Lofreq<-"figure 6"
lower<-"figure 7"
Lower<-"Figure 7"
dups<-"figure 8"
Dups<- "Figure 8"
Table<-"Table 1"
table<-"table 1"
```


Accurately identifying rare single nucleotide variants depends on two components, sensitivity and specificity. Sensitivity, also known as the true positive rate, is the proportion of true variants that are found. Specificity, on the other hand, is the proportion of true negatives that are correctly identified. $1-\text{specificity}$ is the false positive rate, the ratio of false positive variants to true negatives. Both a sensitivity and specificity of 1 describe perfect accurarcy in which all the true variants are found and nothing else.

There is an obvious trade off between sensitivity and specificity. Improved sensitivity often requires less strigent criteria in variant calling which reduces specificity. Conversly, increased stringency can improve specificity but reduce sensitivity. This relationship can be represented as a receiver operating characteristic (ROC) curve (`r xamp_roc`). An ROC plots the sensitivity of an assay along the y axis and 1-specificity along the x axis. To make an ROC curve for a variant callling algorithm one must test the assay on known data. The outcomes can then be stratified according to a metric that quantifies the probability that a given variant is real; often this is a p value or quality score. In a controled benchmakring experiment, all true variants are known, and the sensitivity and specificity can be calculated at different cut offs (`r xamp_roc` A). These points are then used to construct the curve (`r xamp_roc` B). A perfect ROC curve in which all the true positives can be separated from all false positives is a right angle that follows the upper left perimeter of the plot.

#### Initial accuracy

It is beyond the scope of this work to comprehensively compare all variant calling methods virologists use. I Instead we focus our efforts towards robustly benchmarking one variant caller and highlighting a few means for improving accuracy. In the process we demonstrate the importance of robustly benchmarking any variant calling pipeline under the experimental conditions to which it is being applied. 
 
 We have chosen DeepSNV as our variant caller of choice for this analysis. DeepSNV uses a clonal, plasmid-derived, control to estimate local error rates controlling for sequence context accross the genome. Because the control is processed together with the experimental samples it has identical noise characteristics. DeepSNV applies a hierarchiacal binomial model at each genomic position and identifies true positive variants with frequencies greater than expected based on the control. Like many variant callers the accuracy of DeepSNV was initially determined using samples that required mimal PCR amplication. To characterize the accuracy of DeepSNV when applied to RT-amplified RNA virus we creadted difed mixutres of two winfluenza strains, WSN33 and PR8 (`r miseq` A). 

Because these viruses differ at 491 positions, there are 491 true positives at each dilution. On plasmids subserted to limited amplification, DeepSNV ad a sensitivity 0.860 and a specificity of 1 for SNV at 0.1% frequency. Under our realistic expermental conditions and with a slightly more stringent p value cutoff ( p=0.01 vs. p=0.05) we found a moderate reducttion in sensitivity  (0.830 for variants at 0.63% and 0.14 for variants at 0.16%). Our specificity was above 0.998 in all dilution. While this reduction in specificity appears minor, a false positive rate of 0.002 applied to the xx,xxx potential variants in the 13.xkb influenza genome results in 78 false positives.

#### An experimental intrahost population

Although our first benchmarking experiment validated our ability to accurately detect rare variants in influenza populations, the experiment was run under relatively artificial conditions. The virus populations found in patient-derived samples are expected to be less divergent than the two lab strains. Also the nucleic acid concentration in patient samples is much lower than that found samples grown in cell culture. To mimic these conditions, we generated 20 viral colones, each with a single point mutation in the WSN33 background. We sequenced these mutants on the Illumina Miseq platform and cofirmed that no additional mutations >1% frequency had arisen between transfection and the passage 1 stock. We also quantified the genome copy number of each stock using quantitative PCR. We then mixed equal genome equivalents of these 20 virsuses, generating an artificial population of $10^5$ copies per microliter in which each mutation was present at 5% frequency. This population was serially diluted into a stock of WSN33, generating artificial populations witha ll 20 mutations present a 2,1,0.5,0.2, and 0.1% frequency (`r hiseq` A). We then serially diluted each of these $10^5$ copy number populations into basal media to obtain populations with lower input. The range ($10^3-10^5$ copies per microliter) matches the inpus typically found in pateint-derived samples (data not shown). We sequenced the samples on the Illumina Hiseq platform, and variants were called using DeepSNV. We also processed and sequenced the WSN33 stock in duplicate. 

We considered the 20 mutations present in our initial viral mixture as the only true positives in our analysis. Any variant that was present in the wild type stock was masked and neither considered a true positive nor a true negative. This was done to avoid benchmarking DeepSNV with variants identified only be DeepSNV.

The samples with the highest input concentration had the best sensitivity (`r hiseq` B and C). Sensitivity dropped in the 0.5% frequency samples. In general, our sensitivity was lower than that seen previously in the cell culture samples where the input concentration was at least $10^7$ copies per microliter. The drop in sensitivity is likely due to the 100 fold decrease in variant genomes present in the $10^5$ samples. During library prep, a disproportionate amount of the 2.0% sample was lost compared to the others, which likely accounts for the reduced sensitivity relative to the 1.0% sample. 

In addition to a more drastic drop in sensitivity we found a much larger number of false positives at a p value cut off of 0.01 (`r hiseq` C). It should be noted that in all cases the specificity was greater than 99%. However, given the large number of potential false positives in an influenza genome (>40,000) even a specificity of 99% results in well over 200 erroneous variant calls. The drop in specificity is likely due to the decease in input levels. In our experience samples grown in cell culture have at least $10^7$ genomes/$\mu$l. As the genomic input decreases, we are more and more reliant on RT-PCR to amplify the true positive "signal" to a detectable level. It is not surprising that is error prone method also amplifies the noise in the process. Initially it is tempting to increase specificity by simply applying a more stringent p value cut off. This would result in marching backwards towards the y axis along the ROCs in `r hiseq` B. However, it is apparent from  `r hiseq` B that any increase in specificity would result in a drastic decrease in sensitivity. It is clear from `r hiseq` that under experimental conditions the DeepSNV output alone is insufficient to accurately separate true positives from false positives.

In deep sequencing studies of viral diversity mapping quality and base quality cutoffs are often used to ensure that only the highest caliber sequencing data is used to call variants. Mapping quality refers to our confidence that a given read is mapped to the correct position in the genome while base quality estimates the likelihood that the base read by the sequencer is correct. In our initial analysis we masked bases that had a base quality score less than 30. We did not apply any mapping quality cut offs. `r Qual` A shows the distribution of the mean quality scores of the variants (p <0.01) present in the $10^5$ genomes/$\mu$l samples. In our case we find that while true and false positive variants can largely be separated by these metrics,  commonly used cut offs are not sufficient (dashed lines `r qual` A). Additionally, we found that in general false positives were biased near the ends of sequencing reads (`r qual` B). This is not surprising as it is well known that sequencing quality dips near the end of a read; however, recall that all bases with phred scores less than 30 were masked from our variant calling. In this case it seems that even high quality false positives are disproportionately called near the ends of reads, while true positives are called uniformly across the read resulting in an average read position near the middle of the read. (_do I need potential reasons for the bias? locations in the genome...)_

In the light of these quality distributions we applied a number of cut offs and drastically increased our specificity without sacrificing sensitivity (`r qual` C and D). For a variant to be considered in our analysis we required a mean mapping quality of 30, a mean phred score of 35, and an average read position within the middle 50% of the read length. _These cut offs were choosen as they ..._ By applying these quality controls we were able to greatly decrease the number of false positives from over 200 in many cases to an average of 9 without sacrificing a single true positive. We recognize that by applying cut offs after variants have been called we are treating potential variant bases more stringently than those matching the consensus sequence. However,because we are applying cut offs to mean quality scores and there are many more consensus bases than variant bases at each position, it is unlikely these biases have any noticable affect on our analysis. In addition to accurately identifying the presence of expected variants DeepSNV was able to  accurately measure the frequency of these variants (`r freq`). However, while the median frequencies are near the expected values, there is a fair spread about each median, something that should be kept in mind when basing down stream analysis on frequency measurements.

#### Lofreq

In addition to DeepSNV, we analyzed our $10^5$ genomes/$\mu$l influenza populations with the variant caller Lofreq. This was done to ensure that the decreased accuracy that accompanied our realistic experimental conditions was not simply based on the DeepSNV variant calling method. Lofreq had marginally decreased sensitivity compared DeepSNV when applied to the variant frequencies $\geq$ 1.0% but marginally increased sensitivity when applied to variant frequencies $\l$ 1.0% (`r lofreq`). Additionally, Lofreq had specificity near that seen in our cell culture benchmarking experiment, and much better than DeepSNV prior to our quality controls. This is not completely surprising as the Lofreq algorithm takes MapQ and Phred scores into account when calling variants, something DeepSNV does not do. Additionally, Lofreq was not prone to calling false positives near the ends of reads, _(verify whether or not it uses read position Show distributions of Lofreq variants?)_, but   `r lofreq` A suggests that sacrificing sensitivity is the only means of increasing specificity.

Lofreq has been reported to have perfect specificity and a sensitivity of 96% for variants present at 0.2%. However, this accuracy reflects a best case scenario using simulated sequencing reads. As was the case with DeepSNV, the accuracy of Lofreq was much lower when applied under conditions that often face virologist using patient derived samples. Our data suggests that is inappropriate to apply Lofreq or DeepSNV under diverse experimental conditions and expect the performance to be unaffected. 

It may be possible to further filter the putative variants called by Lofreq and achieve a similar accuracy to what was achieved using DeepSNV. However, because DeepSNV was much less computationally intensive, more user friendly _not sure if I can or should say that_, and capable of high accuracy, we continued to benchmark DeepSNV using the less concentrated samples from our dilution series.

_Include Lofreq in supplimentary?_

#### Lower input levels

Influenza patient samples vary in input concentration by several orders of magnitude and depend on a variety of factors often outside the control of a researcher (day of collection, volume of mucus collected ect.). To ensure accuracy across a range of input levels we diluted our known mutant mixes serially into viral media. We found both decreases in sensitivity and specificity accompanied lower input concentrations (`r lower`). The decreased sensitivity is expected as the likelihood of detecting rare events decreases with sample sizem while the decreased specificity is most likely due to a greater dependence on RT-PCR to amplify the signal to detectable limits. Is it not surprising that an amplification of noise accompanies this dependence. However, our results do highlight the importance of controlling for input levels when comparing populations of RNA viruses. 

If RT-PCR errors are responsible for the decreased specificity found at lower input levels, and RT_PCR are randomly distributed across the genome, we would expect such errors to be eliminated by processing the samples in duplicate. To test this hypothesis, a subset of the $10^4$ genomes/$\mu$l were run in duplicate. The 0.2% was not run as we expected a decrease in sensitivity would accompany this approach and had little hope of finding these rare variants based on the sensitivity we saw in `r lower` B. Two RT-PCR reactions were run from a new RNA prep. These samples were processed separately and sequenced on the same lane of an Illumina Hiseq. The previously mentioned quality cut offs were applied and for a variant call to be considered it was required to be found in both duplicates. This drastically improved our specificity and resulted an accuracy that was comparable to that found in the $10^5$ samples (`r dups`). Unexpectedly, an increase in sensitivity accompanied this sequencing run. This is most likely do to a superior library prep with less sample loss during size selection. 












