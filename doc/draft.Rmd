---
output: 
  word_document:
    reference_docx: style.docx


---

```{r message=F,echo=FALSE,cache=T,results='hide',warning=FALSE}
xamp_roc<-"figure 1"
Xamp_roc<-"Figure 1"
miseq<-"figure 2"
Miseq<-"Figure 2"
hiseq<-"figure 3"
Hiseq<-"Figure 3"
qual<-"figure 4"
Qual<-"Figure 4"
freq<-"figure 5"
Freq<-"Figure 5"
lofreq<-"Figure 6"
Lofreq<-"Figure 6"
lower<-"figure 7"
Lower<-"Figure 7"
dups<-"figure 8"
Dups<- "Figure 8"
Table<-"Table 1"

require("Biostrings")
require(plyr)
 
knitr::knit("../scripts/figures.Rmd",tangle = T)
source("./figures.R")


#This is to reset the cache
# 
# bibliography: bib.bib
# csl: journal-of-virology.csl



```

```{r,message=F,echo=FALSE,cache=T}

miseq_vars<-formatC(miseq_vars,format="d",big.mark = ",")
miseq_kb<-formatC(miseq_kb*1000,format="d",big.mark = ",")

hiseq_vars<-formatC(hiseq_vars,format="d",big.mark = ",")
hiseq_kb<-formatC(hiseq_kb*1000,format="d",big.mark = ",")

st_error="XXX"
false_p_phred<-round(false_p_phred,digits=1)
corr_coef<-round(corr_coef,digits=4)
mean_fold<-round(mean_fold,digits=2)*100
read_depth<-"a lot"
#print(interest)
#csl: journal-of-virology.csl
```

```{r,echo=F}
 ####################### To do List #######################
# Is it clear that I'm only talking about the 10^5 samples until we get to the lower input section?

#Mapq exploration - concordant vs nonconcordant pairs

# add mfe raw data to repo. currently pulls data from desktop directory.

# add WSN33 and PR8 consensus sequences to repo

# plot of read start postion vs. genome
# do we not find the same variants
# L1-norm of 4_5 and 5_5
```

**Validation and quality controls in next generation sequencing based studies of intrahost diversity**

\n

\n

\n

John T. McCrone^1^ and Adam S. Lauring^1,2^ *


\n

\n

1 Department of Microbiology and Immunology, University of Michigan, Ann Arbor, MI 48109

2 Division of Infectious Diseases, Department of Internal Medicine, University of Michigan, Ann Arbor, MI 48109


\n

\n

\n

\n


\n

\n

\n
*Corresponding author

\n



Adam S. Lauring

1150 W. Medical Center Dr.

MSRB1 Room 5510B

Ann Arbor, MI 48109-5680

alauring@med.umich.edu




#Introduction

The advent of next generation sequencing (NGS) has enabled detailed characterization of intrahost diversity and has revolutionized studies of  viral evolution and molecular epidemiology. Until recently, investigations of viral diversity were limited by labor intensive sequencing approaches. However, it is now feasible to sequence patient-derived samples at sufficient read depth to detect rare single nucleotide variants (SNV), and the cost-effectiveness of NGS has led to an explosion of studies that quantify viral diversity within and between hosts (e.g.[@Andersen:2015jj], [@Grubaugh:2015by], [@Rogers:2015ij],[@Poon:2016gf]). Given the increasing contribution of NGS to the field of virology, it is important to reflect on the accuracy of these methods and the controls that are needed to ensure robust conclusions.


Because next generation sequencing of viral populations often produces tens of thousands of reads per genomic position, the sensitivity of these approaches has been assumed to be quite high (cite). However, patient-derived samples can differ in input titer by orders of magnitude (e.g. [@Lau:2013ima],[@Teunis:2015iw],[@Takeyama:2015jg]), and this variability can have drastic impacts on the sensitivity of detection for rare events. Furthermore, some library preparations result in variable sequencing coverage both within and between samples. These sources of ascertainment bias can be easily  overlooked in NGS based studies of viral diversity.

The specificity of NGS is another factor to consider when designing an experiment using patient-derived samples. Each sequencing platform has inherent error profiles (cite), and sample collection, target amplification, and library preparation are additional processes whereby errors can be introduced and propagated.The next generation sequencing of RNA virus populations is particularly error prone, and these errors must be accounted for to ensure accurate secondary analysis.

Many sample preparation protocols have been developed to control for errors in NGS-based studies of RNA virus populations. However, each approach has its own caveats that ultimately limit its application. Cirseq is an ingenious approach in which  template RNA is sheared and circularized prior to reverse transcription  [@Acevedo:2014ej]. Subserquent rolling circle cDNA synthesis produces tandem reads, generating a consensus sequence for each RNA fragment. While this method is likely to be highly sensitive for rare variant detection and can control for reverse transcription, PCR, and sequencing errors, the requirement for a large and relatively pure population of viral RNA limits its applicability to patient-derived samples [@Acevedo:2014df]. "Primer ID" based methods require less input and target sequencing to the viral genome. This approach relies upon barcoded primers to construct consensus sequences for each cDNA template and can control for PCR and sequencing errors  [@Jabara:2011eo]. Because Primer ID methods require taht each bar code be physically attached to a PCR product, they  are most easily applied to a small targeted regions of the genome. As such, they have limited application in whole genome sequencing.

An adapted form of sequence independent single primer amplification (SISPA) is an alternative approach that allows for whole genome sequencing and controls for errors propagated during library preparation [@Djikeng:2008cd]. In this method RT-PCR products are sheared and tagged with bar-coded random primers in a klenow reaction, prior to library preparation. SISPA controls for any errors that may arise during the library amplification such as PCR biases. This method has been used in conjunction with statistical algorithms to control for accuracy in studies of intrahost influenza diversity (Ghedin group citations). However, the bar-coding reaction used in SISPA is biased and can result in uneven coverage and sensitivity across the genome  [@Rosseel:2013dn]. 


Given the above limitations, many statistical approaches have been developed to distinguish true variants from sequencing errors [@Gerstung:1js; @Isakov:2015gq; @Wilm:2012br; @Macalalad:2012fx; @Koboldt:2009dk]. In general, these algorithms calculate base-specific error rates from various metrics including but not limited to: mapping quality (MapQ), base quality (Phred), strand bias, and sequence context. True variants are identified as those with frequencies exceeding the expected error rate according to some predetermined statistical test. Despite being employed in many NGS based studies of viral diversity (e.g. ), few of these algorithms have been benchmarked using defined viral populations, and to our knowledge none have been tested under conditions that mimic those found in patient-derived samples. The accuracy of such algorithms in the context of NGS studies of patient-derived viral populations is largely unknown.

Here, we use genetically-defined populations of Influenza A virus and variable input titers to indirectly determine the accuracy of rare variant detection in patient-derived samples. We highlight the challenges that accompany NGS-based studies of viral diversity and include a few means for improved the accuracy. This work exemplifies the controls that should be run prior to any NGS based study of viral populations and provides a comprehensive data set for benchmarking other pipelines.

# Methods

### Viruses and cells

Madin-Darby canine kidney cells were provided by Arnold S. Monto (University of Michigan School of Public Health) and were maintained in Dulbecco's Modified Eagle Medium (DMEM, Invitrogen) with 10% fetal bovine serum (Gibco and HyClone), 25mM HEPES (Invitogen), and 0.1875% bovine serum albumin (Life Technologies). Influenza A/WSN/33(H1N1) virus was rescued from transfected cells using the 8 plasmid reverse genetic system containing the genomic segments (pHW181-188), a kind gift from Robert Webster (St. Jude's Children's Research Hospital) [@Hoffmann:2002ih,@Pauly:2015eq]. A biological clone of influenza A/Puerto Rico/8/1934(H1N1) was obtained from ATCC (VR-1469) and the genomic segments were cloned into the pHW2000 reverse genetic system [@Pauly:2015eq].


### Viral populations

We extracted viral RNA from infected supernaturants using  QIAamp Viral RNA kits (Qiagen 5204) and generated cDNA using Superscript III one-step with HiFi platinum Taq (Invitrogen 12574). PCR products were purified using the GeneJet PCR Purification Kit (ThermoFisher Scientific K0701) according to the manufactures' instructions.

#### PR8-WSN33 population 

For the experiment in `r Miseq`, WSN33 and PR8 viruses were plaque purified and passaged three times in MDCK cells. We then verified the sequences of the these viruses by Sanger sequencing.  Two microliters of RNA template were used  to generate cDNA in eight segment-specific one-step RT-PCR with $0.2\mu\text{M}$ of the following primers :   PB2-Forward-JT	(5'-GCAGGTCAATTATATTCAATATGGAAA-3'), PB2-Reverse-JT	(5'-CAAGGTCGTTTTTAAACTATTCGACAC-3'), PB1-Forward-JT (5'-GCAGGCAAACCATTTGAATGG-3'), PB1-Reverse-JT	(5'-CAAGGCATTTTTTCATGAAGGACAAG-3'), PA-Forward-JT	(GCAGGTACTGATTCAAAATGGAAG-3'), PA-Reverse-JT	(CAAGGTACTTTTTTGGACAGTATGG-3'), NA-Forward-JT	5'-(GCAGGAGTTTAAATGAATCCAAACC-3'), NA-Reverse-JT	(5'-CAAGGAGTTTTTTGAACAAACTACTTG-3'), HA-Forward-JT	(5'-GCAGGGGAAAATAAAAACAACCAAAAT-3'), HA-Reverse-JT	(5'-CAAGGGTGTTTTTCCTTATATTTCTGAA-3'), NP-Forward-JT	(5'-GCAGGGTAGATAATCACTCACAG-3'), NP-Reverse-JT	(5'-CAAGGGTATTTTTCTTTAATTGTCGTACT-3'), M-Forward-JT	(5'-GCAGGTAGATATTGAAAGATGAGTC-3'), M-Reverse-JT	(5'-CAAGGTAGTTTTTTACTCCAGCTCT-3'), NS-Forward-JT	(5'-GCAGGGTGACAAAGACATAATG-3'), NS-Reverse-JT	(5'-CAAAGGGTGTTTTTTATTATTAAATAAGCTG-3'). Reaction conditions were $50^\circ\text{C}$ (60 min), $94^\circ\text{C}$ (2 min), followed by 30 cycles of $94^\circ\text{C}$ (30 sec), $54^\circ\text{C}$ (30 sec), and $68^{\circ}\text{C}$ (3 min). Molar equivalents of each PCR product were pooled to generate reconstituted cDNA genomes of WSN33 and PR8. The WSN33 cDNA pool was then serially diluted into the PR8 cDNA pool to yield WSN33-PR8 mixtures in which WSN33 made up 5, 2.5, 1.25, 0.63, and 0.16\% of the population. Seven hundred and fifty nanograms of each mixture were sheared to an average size of 300-400 bp using a Covaris S220 focused ultrasonicator with the following settings : Intensity: 4, Duty cycle: 10%, Burst/second: 200, Duration: 80 seconds. Sequencing libraries were prepared from these fragmented products using the NEBNext Ultra DNA library prep kit (NEB E7370L), Agencourt AMPure XP beads (Beckman Coulter A63881), and NEBNext multiplex oligonucleotides for Illumina (NEB E7600S). Pooled libraries were sequenced on an Illumina Miseq machine using 2 x 250 paired end reads. A clonal plasmid control library was prepared from 8 plasmids containing PR8 genomic segments. These plasmids were mixed to equal molarity, and cDNA was generated using a multiplex one step RT-PCR with the primers Uni12/Inf1 (5'-GGGGGGAGCAAAAGCAGG-3'), Uni12/Inf3 (5'-GGGGGAGCGAAAGCAGG-3'), and Uni13/Inf1 (5'-5CGGGTTATTAGTAGAAACAAGG-3') as in [@Anonymous:A25Ka2tY]. This library was prepared in identical fashion to the experimental populations and was sequenced in the same Miseq lane.

#### Experimental intrahost population

Twenty point mutants were generated in the WSN33 background using the pHW2000 reverse genetics system [@Hoffmann:2002ih] (Visher et al., manuscript in preparation). In short, we used  overlap PCR mutagenesis to introduce the following mutations: HA\_T1583G, HA\_G1006T, HA\_G542T, M\_T861G, M\_A541C, NA\_G1168T, NA\_C454T, NP\_A454C, NP\_A1160T, NS\_G227T, NS\_A809G, PA\_T964G, PA\_T237A, PA\_A1358T, PB1\_G599A, PB1\_G1764T, PB1\_T1288A, PB2\_A1854G, PB2\_A440T, PB2\_A1167T. Viruses were rescued from transfected cells as in [@Pauly:2015eq].

We passaged the 20 WSN33 point mutants and the WSN33 WT once in MDCK cells and verified the identity and relative clonality of the mutants by sequencing each on an Illumina Miseq as above. We quantified the genome copy number of each supernatant using a SuperScript III Platinum One-Step qRT-PCR kit (Invitrogen 11732) and universal influenza A/B primer and probe sets [@Anonymous:tc]. Equal genome equivalents of each infected supernatant were mixed  and diluted to generate a population with each mutant present at 5% frequency and a total concentration of 10^5^ genomes per microliter. We diluted this mixture into WT WSN33 supernatant to create populations in which each mutant was present at 2, 1, 0.5,and 0.2% frequency all with a total concentration of 10^5^ genomes per micoliter. These 5 populations were diluted serially into basal media to generate samples with total nucleic acid concentrations of 10^4^ and 10^3^ genomes per microliter. Viral RNA was extracted from these samples and cDNA was generated in a one step multiplex RT-PCR as above. The WT WSN33 sample (10^5^ genomes per micoliter) was processed and sequenced in duplicate. We prepared libraries as before and used Quanti PicoGreen dsDNA quantification (ThermoFisher Scientific P7589) to quantify the concentration of each indexed library. We pooled equal quantities (by nanogram) of each indexed library, and removed adaptor dimers by gel isolation with the GeneJET Gel Extraction Kit (ThermoFisher Scientific K0691) prior to sequencing an on Illumina Hiseq 2500 with 2 x 125 paired end reads. A clonal control library was processed in an identical fashion starting from an equimolar mix of 8 plasmids containing the WSN33 genomic segments. 


Fresh RNA was isolated from the 5%, 2%, 1%, and 0.5% 10^4^ genomes per microliter samples and processed in duplicate as above, and these duplicates were sequenced with a plasmid control using 2 x 125 paired end reads on an Illumina Hiseq 2500 machine.


### Sequence analysis

 Reads were aligned to either a PR8 or a WSN33 reference sequence using Bowtie2 [@Langmead:2012jh]. Alignments were sorted and PCR duplicateds removed using Picard (http://broadinstitute.github.io/picard/). Variants were called using either DeepSNV [@Gerstung:1js] or Lofreq [@Wilm:2012br] and filtered using the Pysam module in python (https://github.com/pysam-developers/pysam) and custom R scripts. Bases with a Phred<30 were masked in the DeepSNV analysis. We connected all of these steps into an analysis pipeline using bpipe [@em:2012wd] which is available for download at https://github.com/lauringlab/variant\_pipeline.  To save memory during SNV processing only variants with p<0.9 were included in our ROC curve analysis, as the vast majority of true negatives are trivial to identify and have a p=1. For ease of viewing, and to account for this analysis artifact, we extended the ROC curves horizontally from the last observed change in sensitivity. All raw fastq files were submitted to the Sequence Read Archive (SRA) under accession number ___________, and all commands required generate the figures in the manuscript are available for anonymous download at https://github.com/lauringlab/Benchmarking\_paper . Finally an interactive shiny app of our  benchmarking work can be 
downloaded at https://github.com/lauringlab/benchmarking\_shiny. 

##Results

The ability to reliably identify single nucleotide variants (SNV) is integral to accurate NGS-based studies of viral diversity. The accuracy of any SNV calling pipeline can be described in terms of its sensitivity and specificity. Sensitivity is the proportion of true variants that are properly identified, and specificity is the proportion of true negatives that are properly identified. A closely related concept is the false positive rate, which is 1-specificity, and gives the ratio of false positive variants to true negatives. An assay with perfect accuracy, in which all the true variants are found and only true variants are found, has a sensitivity and specificity of 1.

There is an obvious trade-off between sensitivity and specificity. Improved sensitivity often requires less stringent criteria in variant calling, but reduces specificity. Conversely, increased stringency can improve specificity but often reduces sensitivity. This relationship can be visualized using a receiver operating characteristic (ROC) curve (`r Xamp_roc`). An ROC curve plots the sensitivity of an assay along the y-axis and 1-specificity, or the false positive rate, along the x-axis. A variant calling pipeline must be tested against known data in order to construct an ROC curve. The outcomes can then be stratified according to a metric that quantifies the probability that a given variant is real, often a p value or quality score. In a controlled benchmarking experiment, all true variants are known, and the sensitivity and specificity can be calculated at different cut-offs (`r Xamp_roc`A). These points are then used to construct the curve (`r Xamp_roc`B). A perfect ROC curve in which all the true positives can be separated from all false positives is a right angle that follows the upper left perimeter of the plot.


### Initial accuracy

A comprehensive comparison of SNV calling approaches is beyond the scope of this work. Instead, we robustly benchmark one variant caller, DeepSNV, and highlight approaches for improving its accuracy. In doing so we demonstrate the importance of robustly benchmarking any variant calling method under the experimental conditions to which it is applied. 
 
DeepSNV is a variant calling algorithm that uses a clonal, plasmid-derived, control to estimate local error rates across the genome [@Gerstung:1js]. Because it is clonal, the sequence of the control is known with a high degree of confidence, and any nonconsensus base is indicative of an error. Additionally, the control and experimental samples are processed together and are assumed to have identical noise characteristics. DeepSNV then applies a hierarchical binomial model at each genomic position and identifies true variants as those with frequencies significantly above the noise found in the plasmid control. Like many variant calling algorithms, the accuracy of DeepSNV was initially determined using samples that required minimal PCR amplification. However, its accuracy has not been tested when applied to whole genome sequencing of a viral population amplified by reverse transcription PCR (RT-PCR). 

In our first benchmarking data set we created defined mixtures of two influenza strains,WSN33 and PR8. We plaque purified and expanded WSN33 and PR8 populations. Complementary DNA from both viruses were mixed serially such that WSN33 cDNA was present at frequencies of 5, 2.5, 1.25, 0.63 and 0.16\% (`r Miseq`A). These viruses differ at 491 positions (primer sites used in RT-PCR were excluded from analysis), providing 491 true positives in each dilution. On plasmids subjected to limited PCR, DeepSNV identified known variants at 0.1\% frequency with a sensitivity of 0.860 and  specificity of 1.0 [@Gerstung:1js]. Under our experimental conditions and with a slightly more stringent p value (p=0.01 vs. p=0.05) we found a moderate reduction in sensitivity  (`r miseq.sense.63` for variants at 0.63\% and `r miseq.sense.16` for variants at 0.16\%). The specificity was above 0.998 in all dilutions. While this drop in specificity appears small, a false positive rate of 0.002 corresponds to 78 false positives when applied across the `r miseq_vars` potential variants present in the `r miseq_kb` bp of the influenza genome.

###An experimental intrahost population

Although the initial benchmarking experiment validated our ability to accurately detect rare variants in influenza populations, the experiment was run under relatively artificial conditions. Patient-derived populations are typically less divergent than WSN33 and PR8 (cite Ghedin papers), and the number of viral genomes in patient-samples is much lower than that found in cell culture. To mimic these conditions, we generated 20 viral clones, each with a single point mutation in the WSN33 background. We sequenced these mutants on the Illumina Miseq platform to account for any additional mutations that might have arisen between transfection and the passage 1 stock. Four additional mutations were found above 1% frequency (frequencies 1.2%-3.7%). We also determined the genome copy number of each stock using quantitative RT-PCR. We then mixed equal genome equivalents of these 20 viruses, to generate an artificial population with $10^5$ copies per microliter with each mutation  present at 5\% frequency. This population was serially diluted into a stock of WSN33, generating artificial populations with all 20 mutations present a 2, 1, 0.5, 0.2, and 0.1\% frequency (`r Hiseq` A). We then serially diluted these populations into basal media to obtain mixtures with lower nucleic acid input. The range (10^3^-10^5^ copies per microliter) matches the inputs typically found in patient-derived influenza virus samples (data not shown). We sequenced the samples on the Illumina Hiseq platform, and called variants using DeepSNV. We also processed and sequenced the WSN33 stock in duplicate to control for any mutations in the viral dilutent. 

The 20 mutations present in our initial viral mixture were the only true positives considered in our analysis. Any SNV that was present at >1% frequency in either in both duplicates of the wild type stock or any one of the viral clones was masked excluded from the analysis and  considered neither a true positive nor a true negative. 

In these populations with lower diversity and input titer, we maintained greater than 0.85 sensitivity for SNV at 1% frequency or higher. Despite a high depth of coverage (>10,000 reads per bp), our sensitivity was considerably lower for variants at or below 0.5% frequency (`r Hiseq`B and C). The drop in sensitivity is most likely due to the 1,000-fold decrease in nucleic acid concentration compared to the first data set, and the fact that library preparation requires a number of sampling steps that may limit detection. During library preparation, a disproportionate amount of the 2.0\% sample was lost compared to the others, which likely accounts for the reduced sensitivity relative to the 1.0\% sample. 

In our initial analysis of these data using the same default DeepSNV settings as above, the specificity was significantly lower, 0.xxx. These lower input samples underwent more PCR cycles, which skews the noise distribution of variant calls in the test vs. plasmid control samples. Therefore, we utilized an alternative beta binomial model available in DeepSNV, which more appropriately models these differences (cite). With these settings, the specificity was greater than 0.99 in all the samples with 10^5^ genomes per microliter. As above, while 0.99 specificity appears robust, a false positive rate of 0.01 results in over 200 false positive variants when applied to the `r hiseq_vars` potential variants in the influenza genome. The false positives outnumber the true positives by 100-fold in these populations with realistic diversity and input. We were able to increased our specificity  by applying a more stringent p-value cutoff. However, as shown by the ROC curves in `r Hiseq`, this move towards the y axis markedly reduces sensitivity. Our data demonstrate that with moderate concentrations of input nucleic acid, even statistically significant p-values from a robust variant caller are not sufficient to accurately separate true from false positive variants.

###Additional filtering criteria

Many next generation sequencing studies utilize mapping quality (MapQ) and/or base quality (Phred) cut-offs to ensure that only the highest caliber sequencing data is used to call variants. Mapping quality measures the probability that a given read is mapped to the correct position in the genome while base quality estimates the likelihood that the base call by the sequencer is correct. In the above analysis we masked bases that had a Phred  score less than 30 (0.001 probability of being incorrect) and did not apply any MapQ cut-offs. In our next analysis analysis, we applied frequently employed cut-offs such as a MapQ of 20 and a Phred of 30 to our data. These criteria were unable to distinguish true from false positives in our 10^5^ samples (`r Qual`A) and indicate that many false positives occur on well mapped reads with high quality base calls.

We further parsed our false variant calls by locating them within individual sequencing reads. It is well known that sequence quality drops near the end of a read (cite), and we found that our false positives clustered at the termini of our paired end reads (`r Qual`B). The average Phred score of these false positives was `r false_p_phred`, further demonstrating that filtering on quality score alone is insufficient. In contrast, true positives were uniformly distributed across the reads resulting in an average read position near the middle of the read. 


Based on these results, we applied a number of empirically determined cut-offs and markedly improved our specificity to >0.999 without sacrificing sensitivity (`r Qual`C and 4D). For a variant to be considered in our analysis we required a mean mapping quality of 30, a mean Phred score of 35, and an average read position within the middle 50\% of the read. Under these conditions we found 10 or fewer false positives in 4 of the 5 samples. Given this success we applied a number of other strategies to further increase our accuracy, including but not limited to: Benjamini Hochberg p-value correction,  more stringent p values (<0.01) or frequency cut-offs (>0.2%), retention of duplicate PCR reads, trimming the ends of the influenza genome, and employing alternative statistical distributions to estimate the error rate in the control sample. None of these approaches significanlty imporved our accuracy over teh above qualitya nd read-position criteria. The impact of various filtering criteria on our data can be visualized in an interactive shiny application available for download at  https://github.com/lauringlab/benchmarking_shiny.git.

We also benchmarked the accuracy of our DeepSNV pipeline in estimating the frequency of the true positive variants (`r Freq`). Although the median of the measured frequencies match the expected values, we found substantial spread in each sample and the overall the fit was quite poor ( R^2^=`r corr_coef`). The mean percent difference between the measured frequency and the expected was `r mean_fold`\%. This error is likely due to stochastic sampling errors associated with the RT-PCR and library preparation (cite?) and should be kept in mind when employing down stream analysis that depend on frequency measurements (e.g. variant fitness, haplotype reconstruction, Shannon's Entropy, and other diversity metrics). 

###Alternative variant callers

DeepSNV is one of many variant callers that employ a combination of empiric and statistical approaches to model error rates. We asked whether the decreased accuracy observed in our data set was due simply to peculiarities specific to  DeepSNV. We therefore analyzed our $10^5$ input populations using Lofreq, another variant caller that is commonly used in next generation sequencing studies, and has been reported to have perfect specificity [@Wilm:2012br]. Under our experimental conditions, Lofreq had marginally reduced sensitivity compared to DeepSNV when applied to the variant frequencies $\geq$ 1.0\% but marginally increased sensitivity when applied to variant frequencies $<$ 1.0\% (`r Lofreq`). The specificity of LoFreq was comparable to what we observed with DeepSNV in our high-input cell culture-derived populations (`r Miseq`), and better than DeepSNV in our initial implementation (`r Hiseq` prior to Phred, MapQ, and read position filtering). This increased specificity is most likely due to the fact that the LoFreq algorithm already takes MapQ and Phred scores into account when calling variants and has a stringent strand bias filter that removes many of the variants found only at one end of a paired-end read. However, even with these additional characteristics, the specificity of LoFreq was lower than our improved DeepSNV pipeline (compare `r Lofreq` and `r Qual`), with over 40 false positives per sample. It appears that higher than expected false positive rates are not specific to DeepSNV and most likely plague many variant callers applied to patient-derived viral samples.


###Accuracy at Lower input levels

Host-derived viral populations vary in copy number and titer by several orders of magnitude. This variability can be attributed to a variety of factors including, but not limited to: collection site, ease of of nucleic acid isolation, the presence of host nucleic acid, efficiency of library preparation, and host and viral factors. To ensure accuracy across a range of input levels, we diluted our experimental populations serially into basal media (`r Hiseq`A) and identified variants using our modified DeepSNV analysis pipeline (`r Lower`). As expected, our sensitivity was lower in populations with fewer genomes. For example, a variant with a 0.5\% frequency in a $10^4$ genomes per microliter sample is expected to be present on only 700 genomes in the initial RT-PCR. Many of these will be lost due to bottlenecks in the amplification and library preparation process. W also found reduced specificity in samples with lower starting input. The increase in false positives is presumably due to a greater dependence on RT-PCR amplification and consequent propagation of errors. These data highlight the importance of controlling for input levels when comparing diversity across experimental samples.

In most cases, RT-PCR errors should be sporadic and randomly distributed across the amplified region. If RT-PCR errors are responsible for the reduced specificity found at lower input levels, they should be easily identified as variants present in only one of two RT-PCR reactions performed on the same RNA. To test this hypothesis, we  sequenced duplicate RT-PCR reactions of the  5, 2, 1, and 0.5\% variant frequency samples from our $10^4$ genomes per microliter collection. The duplicates were processed separately, but sequenced on the same lane of an Illumina Hiseq. We applied the previously mentioned quality cut-offs and required that a given variant be found in both duplicates. By analyizing samples in duplicate, we reduced the number of false positives in each sample to 10 or fewer resulting in and a specificity of >0.9998 (`r Dups`). This increased specificity was not accompanied by a decreased sensitivity. In fact, we found a slight increase in sensitivity, most likely due to variability in library preparation. Thus, accurate analysis of low input samples can be achieved through duplicate RT-PCR and careful benchmarking experiments.

###Sub-optimal SNV identification confounds diversity measurements

NGS of intrahost populations is commonly used determine the interaction between host or environmental factors and viral diversity. Because measurements of viral diversity rely entirely on SNV identified in NGS data, they are very sensitive to the accuracy of these variant calls. To illustrate this problem, we calculated the diversity of our 10^5^ genomes per microliter samples at each step of our benchmarking process using three complementary metrics (`r Table`). Richness is the count of nonconsensus variants present in a population (often referred to as intrahost SNV). Shannon's Entropy is a diversity metric that accounts for  both the number of variants present (richness) and their frequencies (evenness). Because our data are unphased (i.e. no haplotypes), we have reported the average entropy per nucleotide position (cite). The last metric, L1-norm, is a distance measurement that describes how similar two populations are to one another. Identical populations will have an L1-norm of 0. To mimic experimental conditions, we included all variants identified in each analysis regardless of whether or not subsequent benchmarking distinguished them as true or false positives. We found that the accuracy of the SNV calling method has a profound effect on measurements of diversity. It is clear from the richness measurements in `r Table` that the number of false SNV (i.e. the specificity) largely determines the accuracy of the down stream anlaysis. Thus, our adapted DeepSNV protocol, which was able to distinguish between true and false SNV with the highest accuracy, gave the most accurate measures of diversity, followed by Lofreq and the default version of DeepSNV.


##Discussion

Robust validation is essential in NGS-based studies of viral diversity. Differences in experimental design and sample preparation can lead to wide variability in the accuracy of SNV indentification. We found that input nucleic acid concentration, which can vary greatly in patient-derived samples, had a large impact on both the sensitivity and specificity of rare variant detection. At moderate levels of nucleic acid input we could improve accuracy by filtering putative SNV based on quality metrics and read position. We further improved our accuracy at low input levels by processing these samples in duplicate. While our quality cut-offs may not be universally applicable to all samples and variant callers, our data sugest that experimental design is critical for accurate SNV detection. These findings are important as few, if any, variant callers have been benchmarked under patient-derived conditions. Finally, we show that these inaccuracies in SNV calling drastically impact downstream analysis.


We chose DeepSNV for our studies, because it is one of the only variant callers that has been validated on real sequencing reads in which all true positive variants and their frequencies were known *a priori* and independent of NGS (cite Gerstung). In our experimental conditions we found decreased sensitivity and specificity when applied to cell culture derived mixtures of WSN33 and PR8. The most striking drop in sensitivity came for variants found at 0.16%. Under these conditions we found true variants at a rate of `r miseq.sense.16` compared the expected sensitivity of  0.860 for variants at 0.1%. This decrease is most likely due to the fact that we are looking for variants across `r miseq_kb`bp of the influenza genome where previous benchmarking focused on just 1,500 bp of the HIV pol gene. Therefore, our sequencing efforts were diluted across the influenza genome resulting in decreased read depth  which limits sensitivity. Additionally,  we aligned all reads to a PR8 reference genome which biases our detection of WSN33 variants especially rare variants, which already have few reads. We also observed a relatively modest decrease in specificity. Despite the use of bonferronni corrected p values, the drop in specificity is most likely due to multiple testing errors caused by applying the algorithm across `r miseq_kb`bp (cite). 
 
 Encouraged by these results, we applied DeepSNV to experimental populations that more closely mimic the diversity and nucleic acid input levels found in patient derived samples. At a modest input concentration of 10^5^ genomes per microliter we found a four times as many false positives. It should be noted that our specificity in this case remained above 0.99 in all populations and that seemly robust specificities can result in an unacceptable number of erroreous SNV when whole genomes are analyzed. The reduced specificity was likely an artifact of how the populations were constructed and stresses the importance of benchmarking SNV identifying pipelines under conditions to which they will be applied. Our WSN3-PR8 populations were constructed from reconstituted cDNA genomes and were devoid of any biases that might accompany PCR amplification. However, this was not the case in our 20 mutant populations. To more accurately mimic the method we use to process patient samples we mixed all 20 point mutants and WSN33 WT supernatants prior to  amplification in a multiplex RT-PCR. This method undoubtedly introduced unavoidable PCR biases into our populations. Gerstung *et. al.* have shown that PCR amplification biases lead to increased variability in the subconsensus nucleotide counts (cite DeepSNV documentation). Increased bariability in the test samples increases the likelihood that an erroneous SNV will exceed the error rate estimated in the plasmid control. DeepSNV attempts to control for such variability in part by estimating the dispersion of a sample, which allows the algorithm to incorporate greater variance into the null model. In fact, our 20 mutant libraries were more dispersed than those in cell culture, indicating a need for greater variance. However, the dispersion estimates are limited and the variability likely exceeded what can be controled by this approach.

Because RT-PCR biases reduced the efficacy of the DeepSNV algorithm, we search for alternative means of increasing accuracy. Out-of-the box DeepSNV is rather agnostic towards the mapping quality and base quality of a given variant, and only masks bases calls with Phred scores below a user designated threshold. We were able to use  quality metrics to filter putative variants and greatly increase the accuracy of SNV identification. However, commonly used thresholds (MapQ>20, Phred>30) did little to increase the accuracy of our method. Instead, we relied on our own empirically determined thresholds. We found the average MapQ and Phred scores of putative SNV were bimodal with true SNV found in the higher of the two distributions. In both the MapQ and Phred distributions there was a clear distinction between the two populations, and we applied quality thresholds to eliminate any putative SNV found in the lower of the two distributions. We found no correlation between MapQ and Phred (p>x), suggesting these metrics are independent and thus both thresholds eliminate distinct false positives. We have sequenced over 300 influenza libraries in more than 5 Hiseq runs and observe consistent quality distributions for DeepSNV identified putative SNV. Our group has also deeply sequenced many polio virus libraries and see the same bimodal trends. However, we do see shifts in the means of our MapQ distributions when comparing polio and influenza libraries. This is most likely due to differences in the genomic structures of the viruses. The terminal regions of all 8 influenza genomic segments are conserved and this conservation decreases the MapQ of reads that map to these areas. Therefore, while our MapQ and Phred thresholds are robust in our system, they may need to be adjusted for use in other systems. 

Even in the face of stringent MapQ and Phred cut-offs, we found many high quality false positive SNV were identified only at the termi of paired end reads. We removed these by filtering putative SNV calls based on their average position in a paired end read. In particular these false positives were found almost exclusively in regions of the genome that were enriched for read start sites. This enrichment is most likely a consequence  of sequence context, the fragmentation process, and our size selection protocol; however, it is unclear why our plasmid control would not contain similar biases. The cause may also have a biological foundation as previous work has shown that defective interfering  particles, which commonly arise during cell passage,contain truncated genomic segments. Such truncated segments would only be present in infected supernatants and not a plasmid control. These  "read position" false positives were reproducibly found across a number of libraries; however, because our true positives were made by overlap PCR mutagenesis we can be confident such false positives are indeed artifacts of the experimental design and/or sample preparation, and  that we have robustly controlled for such errors.

It is common to filter and trim sequencing reads prior to SNV identification. However, we have taken a slightly different approach in our analysis. In the initial SNV identification step we have masked bases with Phred<30 but made no additional restrictions on the raw data. Only after putative SNV (those that exceed the frequency expected given a plasmid control) have been identified do we impose quality restrictions. Our approach treats variant nucleotides more stringently than consensus base calls, but this is of little concern for two reasons. Firstly, we have identified specificity as a large the problem in accurate SNV identification, and so stringently filtering potential false positives seems appropriate. Additional, since there are orders of magnitude more sequenced reads for a consensus base than there are for rare variant bases, and our thresholds are based on mean quality scores, it is unlikely we would remove any concensus bases from analysis.

Frequency thresholds are an additional quality filter that is often applied to SNV after identification. Common thresholds range from 0.1% to 1%; however, there appears to a consensus that SNV identification is unreliable below 0.1%. We did not apply direct frequency thresholds. In our DeepSNV analysis no putative varaints wer found below 0.1% and we found in our case that frequency cut offs limited sensitivity and did not improve specificity. Read depth, or coverage, is another metric that is often used in conjunction with frequency to insure accurate SNV identification. Although we did not apply a direct coverage cut off, DeepSNV has been reported to require a coverage of ten times the reciprocal of frequency for sufficient power to call SNV. This means that a coverage of 1,000 is needed to detect a variant at 1% frequency. In our analysis the lowest coverage for a true positive was `r tp_cov$coverage` (`r round(tp_cov$freq.var*100,digits=1)`% frequency) while the lowest coverage for a false positive variant was `r fp_cov$coverage` (`r round(fp_cov$freq.var*100,digits=1)`% frequency). Read depth is an important variable in NGS based studies of viral diversity and it, with input concentrations determine the power to detect rare variants. If there are large biases in read depth or input levels, two similar populations may appear distinct based solely on ascertainment bias.

  
Despite high accuracy at 10^5^ genomes per microliter we observed a decrease in sensitivity in our 10^3^ genomes per microliter samples most likely due to the lack of power discussed above. However, this drop in sensitivity was accompanied by reduced specificity. At lower nucleic acid concentrations, our SNV pipeline relied more heavily on RT-PCR amplification, which is an error prone process. RT-PCR errors were indistinguishable from true positive variants, as they do not arise from a sequencing errors or bioinformatic artifacts. We were able to limit these errors which are sporadic and random (cite) by processing low input samples in duplicate, and achieved comparable accuracy to what was seen at 10^5^ genomes per microliter. Quantifying and controlling for RT-PCR errors in this way will allow us to accurately compare patient-samples that fall in a wider this input.
 
  
As we have shown, experimental design impacts the accuracy of SNV identifying pipelines and must be considered in NGS based studies of viral diversity. Target amplification, genome structure and nucleic acid concentration are unavoidable sources of error that can vary depending on the virus and sample collection method. The reported accuracy of SNV identifying algorithms is not directly applicable to every experimental design, and as we have shown these deviations can have profound effects on down stream analysis. In studying influenza, we have benefited from an easily accessible reverse genetics system that made for a straight forward mutagenesis based approach to benchmarking. Under our experimental conditions we have been able to greatly improve the accuracy of DeepSNV. Even though the accuracy remains is imperfect, we are now equipped with an understanding of the limitations of our method. There may be more accurate variant callers than DeepSNV, or algorithms better suited for other systems; however, our work with Lofreq stresses the fact that all methods have their limitations and understanding these limitations is vital for accurate science. Next generation sequencing is powerful tool that is just beginning to unlock the complex dynamics of intrahost viral populations. As this tool becomes more and more popular, it is vital we remember that in NGS as in all science, controls are needed to ensure accuracy.   

**Acknowledgements**

We thank Pat Schloss, Mike Imperiale, and Robert Woods for helpful discussion and a critical reading of the manuscript. This work was supported by a Clinician Scientist Development Award from the Doris Duke Charitable Foundation and R01 AI118886, both to ASL. JTM was supported by the Michigan Predoctoral Training Program in Genetics (T32GM007544).

 
#References